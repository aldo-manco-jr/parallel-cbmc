
HOW TO USE MPI & OPENMPI INTO CBMC AND GET JSON FROM VERISMART:

COPY "rapidjson" header only library into the following path of cbmc project:

    /cbmc/src/goto-checker

edit this file:
    
    /cbmc/src/goto-checker/bmc_util.cpp

add these libraries to bmc_util.cpp:

      #include <mpi.h>
      #include "rapidjson/document.h"
      #include "rapidjson/filereadstream.h"
      #include "rapidjson/filewritestream.h"
      #include "rapidjson/stringbuffer.h"
      #include "rapidjson/writer.h"

edit this function of bmc_util.cpp:

    void run_property_decider(
        incremental_goto_checkert::resultt &result,
        propertiest &properties,
        goto_symex_property_decidert &property_decider,
        ui_message_handlert &ui_message_handler,
        std::chrono::duration<double> solver_runtime,
        bool set_pass
    );

use this MPI and OpenMPI code to communicate with python server:

        MPI_Status status;

        MPI_Init(nullptr, nullptr);

        int initial_rank;
        MPI_Comm_rank(MPI_COMM_WORLD, &initial_rank);
        int initial_size;
        MPI_Comm_size(MPI_COMM_WORLD, &initial_size);

        MPI_Info info;
        MPI_Info_create(&info);
        MPI_Info_set(info, "ompi_global_scope", "true");

        char portNameOmpiServer[MPI_MAX_PORT_NAME];

        MPI_Comm interCommunicator;

        if (initial_rank == 0) {
          std::cout << "Looking up name!" << std::endl;
          MPI_Lookup_name("compute", MPI_INFO_NULL, portNameOmpiServer);
          std::cout << "Name lookup done: " << portNameOmpiServer << std::endl;
        }

        MPI_Comm_connect(portNameOmpiServer, info, 0, MPI_COMM_WORLD, &interCommunicator);
        std::cout << "Comm connected!" << std::endl;

        char tmp[3];
        tmp[0]='a';
        tmp[1]='b';
        tmp[2]='c';

        MPI_Send(tmp, 3, MPI_CHAR, 0, 0, interCommunicator);

        //    FILE* fp = fopen("file_tmp.json", "r+b"); // non-Windows use "r"
        //
        //    char writeBuffer[65536];
        //
        //    MPI_Recv(&writeBuffer, 1000, MPI_CHAR, 0, 0, interCommunicator, &status);
        //
        //    Document d;
        //
        //    FileWriteStream os(fp, writeBuffer, sizeof(writeBuffer));
        //    Writer<FileWriteStream> writer(os);
        //
        //    d.Accept(writer);
        //
        //    FileReadStream is(fp, writeBuffer, sizeof(writeBuffer));
        //    d.ParseStream(is);
        //
        //    fclose(fp);
        //
        //    std::cout << writeBuffer << std::endl;
        //    fclose(fp);

        //    std::cout << writeBuffer << std::endl;

        // string

        //    char stringbuffer1[65536];
        //    MPI_Recv(&stringbuffer1, 1000, MPI_CHAR, 0, 0, interCommunicator, &status);
        //
        //    Document d;
        //    d.Parse(stringbuffer1);
        //
        //    // 2. Modify it by DOM.
        //    Value& s = d["s0"];
        //    s.SetInt(s.GetInt() + 1);
        //
        //    // 3. Stringify the DOM
        //    StringBuffer buffer;
        //    Writer<StringBuffer> writer(buffer);
        //    d.Accept(writer);
        //
        //    // Output {"project":"rapidjson","stars":11}
        //    std::cout << buffer.GetString() << std::endl;

        char stringbuffer1[65536];
        MPI_Recv(&stringbuffer1, 1000, MPI_CHAR, 0, 0, interCommunicator, &status);

        Document d;
        d.Parse(stringbuffer1);

        for (Value::ConstMemberIterator itr = d["s0"].MemberBegin(); itr != d["s0"].MemberEnd(); ++itr){
          std::cout << "Member: " << itr->name.GetString() << std::endl;

          Value& s = d["s0"][itr->name.GetString()];
          std::vector<std::vector<int>> vec;
          vec.resize(s.Size());

          for (SizeType i = 0; i<s.Size(); i++){
            const rapidjson::Value &data_vec = s[i];
            for (SizeType j = 0; j < data_vec.Size(); j++){
              vec[i].push_back(data_vec[j].GetInt()+1);
              std::cout << data_vec[j].GetInt() << std::endl;
            }
          }
          vec.clear();
        }

        // 3. Stringify the DOM
        StringBuffer buffer;
        Writer<StringBuffer> writer(buffer);
        d.Accept(writer);

        // Output {"project":"rapidjson","stars":11}
        std::cout << buffer.GetString() << std::endl;
        //std::cout << vec[0][0] << std::endl;
        /*

      MPI_Comm newCommunicator;
      MPI_Intercomm_merge(interCommunicator, 1, &newCommunicator);

      int new_rank;
      MPI_Comm_rank(newCommunicator, &new_rank);
      int new_size;
      MPI_Comm_size(newCommunicator, &new_size);

      std::cout << "Old rank: " << initial_rank << ", new rank: " << new_rank << std::endl;
      std::cout << "Old size: " << initial_size << ", new size: " << new_size << std::endl;

      std::cout << "Disconnecting comm!" << std::endl;
      MPI_Comm_disconnect(&interCommunicator);
      std::cout << "Comm disconnected!" << std::endl;
      */

      MPI_Finalize();



now it's necessary to edit CMakeLists.txt such that MPI & OpenMPI can be used by bmc_util.cpp
we have to edit this file:

      /cbmc/src/goto-checker/CMakeLists.txt

this must be the code:
      
      set(CMAKE_CXX_STANDARD 14)

      set(CMAKE_PREFIX_PATH "/usr/lib/x86_64-linux-gnu/openmpi")

      find_package(MPI REQUIRED)

      # ----------------
      # This is the only thing that made it work
      # ----------------
      set(MPI_C_LIBRARIES "/usr/lib/x86_64-linux-gnu/openmpi/lib/libmpi.so")
      set(MPI_INCLUDE_PATH "/usr/lib/x86_64-linux-gnu/openmpi/include")
      # ----------------
      set(CMAKE_C_FLAGS "${CMAKE_C_FLAGS}" -fopenmp)
      set(GCC_COVERAGE_COMPILE_FLAGS "-Wall -pedantic -lm -O3 -funroll-loops -fopenmp -Wno-error=switch-enum")
      set(CMAKE_CXX_FLAGS "${CMAKE_CXX_FLAGS} ${GCC_COVERAGE_COMPILE_FLAGS}")
      add_definitions(-DOMPI_SKIP_MPICXX)

      file(GLOB_RECURSE sources "*.cpp" "*.h")
      add_library(goto-checker ${sources})

      generic_includes(goto-checker)
      include_directories(${MPI_INCLUDE_PATH})

      target_link_libraries(goto-checker goto-programs goto-symex solvers util xml goto-instrument-lib ${MPI_C_LIBRARIES} MPI::MPI_CXX)

set into the main CMakeLists.txt,

      /cbmc/CMakeLists.txt

 that cmake version required to parallelize cbmc is at least the 3.22:

      cmake_minimum_required(VERSION 3.22)

Check your current version of cmake:

      cmake --version

If it is less than 3.22 then uninstall it:

      sudo apt remove cmake

Then install the latest version of cmake with snap:

      sudo snap install cmake

To use cbmc in a parallel way:

Compile cbmc with latest version of cmake eventually using snap path:

      /snap/bin/cmake -S. -Bbuild -DCMAKE_CXX_COMPILER=g++-9 -DCMAKE_C_COMPILER=gcc-9
      /snap/bin/cmake --build build

Terminal 1 - OpenMPI TCP Port:

      cd /cbmc/build/bin/

      mpirun -np 1  ompi-server --no-daemonize -r + &

      then you will get an output with like this:

            446627841.0;tcp://192.168.176.93,172.17.0.1:56469

      echo "446627841.0;tcp://192.168.176.93,172.17.0.1:56469" > address_ompi_server

Terminal 2 - Server Python:

      move the terminal to the folder where there is the python server file

      copy "address_ompi_server" file into this folder

      mpirun -np 1 --ompi-server file:address_ompi_server python3 server.py

      now master waits for request or results from cbmc slaves

Terminal 3 - CBMC Slave:

      cd /cbmc/build/bin/

      copy a .c file that you want to analyze into this path (ex. peterson.c)

      mpirun -np 1 --ompi-server file:address_ompi_server ./cbmc peterson.c

